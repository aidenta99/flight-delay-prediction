{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction:\n",
    "## Notebook originally created in Kaggle\n",
    "\n",
    "Due to Kaggle's easy way to manage data through datasets this notebook was used for more than one purpose. In part one, one find the code used to create a Random Forest and LGBM regressor. During the process of the assignment some parts have been updated and removed. For example the input data changed whenever a feature was added / removed or altered.\n",
    "\n",
    "Part 2 shows the pytorch code that was used to generate a deep learning neural network, this code was later put into a script file for it to be used on the Lisa Supercomputer.\n",
    "\n",
    "In Part 3 the submission data is processed and able to be used with whichever model needed. Here also the code has been through several stages, using different features and different models.\n",
    "\n",
    "In Part 4 one finds the code to create a Tensorflow deep neural network as well as the Keras tuner to find the optimal parameters. This was originially meant to be used on Lisa, however due to version incompatabilities of cuda we decdied to switch to a pytorch model in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Random Forest and LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:40:36.364828Z",
     "iopub.status.busy": "2021-12-17T11:40:36.363966Z",
     "iopub.status.idle": "2021-12-17T11:40:37.368284Z",
     "shell.execute_reply": "2021-12-17T11:40:37.367461Z",
     "shell.execute_reply.started": "2021-12-17T11:40:36.364783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:40:37.369901Z",
     "iopub.status.busy": "2021-12-17T11:40:37.369683Z",
     "iopub.status.idle": "2021-12-17T11:41:05.800838Z",
     "shell.execute_reply": "2021-12-17T11:41:05.799895Z",
     "shell.execute_reply.started": "2021-12-17T11:40:37.369874Z"
    }
   },
   "outputs": [],
   "source": [
    "DUMMIES = False\n",
    "REMOVE_US = True\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv('../input/aml-project/train_emiel_v3.csv')\n",
    "X_train = train.drop(['id'], axis = 1)\n",
    "X_train_columns = list(X_train.columns)\n",
    "X_train_columns.remove('AIRLINE')\n",
    "X_train_columns_no_target = X_train_columns\n",
    "X_train_columns_no_target.remove('ARRIVAL_DELAY')\n",
    "X_train[X_train_columns_no_target] = preprocessing.StandardScaler().fit_transform(X_train[X_train_columns], y ='ARRIVAL_DELAY')\n",
    "if REMOVE_US:\n",
    "    X_train = X_train[X_train['AIRLINE'] != 'US']\n",
    "\n",
    "if DUMMIES:\n",
    "    X_train = pd.get_dummies(X_train, columns = ['AIRLINE'])\n",
    "y_train = train['ARRIVAL_DELAY']\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T10:59:31.615403Z",
     "iopub.status.busy": "2021-12-17T10:59:31.614323Z",
     "iopub.status.idle": "2021-12-17T10:59:32.994208Z",
     "shell.execute_reply": "2021-12-17T10:59:32.992437Z",
     "shell.execute_reply.started": "2021-12-17T10:59:31.615269Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train = train.dropna().drop(['ARRIVAL_DELAY'], axis=1).values\n",
    "y_train = train.dropna()['ARRIVAL_DELAY'].values\n",
    "X_val = val.dropna().drop(['ARRIVAL_DELAY'], axis=1).values\n",
    "y_val = val.dropna()['ARRIVAL_DELAY'].values\n",
    "rfg = RandomForestRegressor(n_estimators = 200, max_depth = 15, min_weight_fraction_leaf = 0.000, verbose = 1)\n",
    "rfg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:00:56.329322Z",
     "iopub.status.busy": "2021-12-17T11:00:56.326464Z",
     "iopub.status.idle": "2021-12-17T11:08:18.010037Z",
     "shell.execute_reply": "2021-12-17T11:08:18.005167Z",
     "shell.execute_reply.started": "2021-12-17T11:00:56.329236Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the lightgbm model\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#emiel_csv = val.drop(['ARRIVAL_DELAY'], axis=1).values\n",
    "#emiel_csv = val['ARRIVAL_DELAY'].values\n",
    "\n",
    "\n",
    "md_list = [15]\n",
    "msg_list = [0]\n",
    "lr_list = [0.1]\n",
    "n_est = [150]\n",
    "\n",
    "#small manual grid search.\n",
    "#Figuring out if scores would increase when training seperate models per airline\n",
    "airline_dict ={}\n",
    "airline_list = X_train['AIRLINE'].unique()\n",
    "for md in md_list:\n",
    "    nl = 2 ** md\n",
    "    for msg in msg_list:\n",
    "        for lr in lr_list:\n",
    "            for n in n_est:\n",
    "                for airline in airline_list:\n",
    "                    airline_df = X_train[X_train['AIRLINE'] == airline]\n",
    "                    airline_df = airline_df.drop(['AIRLINE'], axis = 1)\n",
    "                    airline_train, airline_val = train_test_split(airline_df, test_size = 0.15)\n",
    "                    \n",
    "                    y_train_airline = airline_train['ARRIVAL_DELAY']\n",
    "                    X_train_airline = airline_train.drop(['ARRIVAL_DELAY'], axis = 1)\n",
    "                    \n",
    "                    y_val_airline = airline_val['ARRIVAL_DELAY']\n",
    "                    X_val_airline = airline_val.drop(['ARRIVAL_DELAY'], axis = 1)\n",
    "                    \n",
    "                    lgbm_reg = lgb.LGBMRegressor(num_leaves = nl, min_split_gain =msg, max_depth = md, \n",
    "                                                 learning_rate =lr, n_estimators= n)\n",
    "                    param_str = 'md {} msg {} lr {} n {}'.format(md,msg,lr,n)\n",
    "                    param_list = [md,msg,lr,n]\n",
    "                    lgbm_reg.fit(X_train_airline, y_train_airline, eval_metric = 'mse')\n",
    "                    \n",
    "                    y_hat = lgbm_reg.predict(X_val_airline)\n",
    "                    mse = mean_squared_error(y_val_airline, y_hat)\n",
    "                    print('For {}, with params: {}, mse is: {}'.format(airline, param_str, mse))\n",
    "                    \n",
    "                    if mse < 100:\n",
    "                        airline_dict[airline] = lgbm_reg\n",
    "                \n",
    "                \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:43:20.433969Z",
     "iopub.status.busy": "2021-12-17T11:43:20.43334Z",
     "iopub.status.idle": "2021-12-17T11:45:21.733424Z",
     "shell.execute_reply": "2021-12-17T11:45:21.732261Z",
     "shell.execute_reply.started": "2021-12-17T11:43:20.433919Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the lightgbm model on all data\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "full_train_data = pd.get_dummies(X_train, columns = ['AIRLINE'])\n",
    "airline_train, airline_val = train_test_split(full_train_data, test_size = 0.15)\n",
    "\n",
    "y_train_airline = full_train_data['ARRIVAL_DELAY']\n",
    "X_train_airline = full_train_data.drop(['ARRIVAL_DELAY'], axis = 1)\n",
    "\n",
    "\n",
    "#fit the model\n",
    "lgbm_reg = lgb.LGBMRegressor(num_leaves = 2**15, min_split_gain =0, max_depth = 15, \n",
    "                             learning_rate =0.1, n_estimators= 150)\n",
    "\n",
    "lgbm_reg.fit(X_train_airline, y_train_airline, eval_metric = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:45:59.649069Z",
     "iopub.status.busy": "2021-12-17T11:45:59.648147Z",
     "iopub.status.idle": "2021-12-17T11:46:07.102655Z",
     "shell.execute_reply": "2021-12-17T11:46:07.101981Z",
     "shell.execute_reply.started": "2021-12-17T11:45:59.649017Z"
    }
   },
   "outputs": [],
   "source": [
    "y_val_airline = airline_val['ARRIVAL_DELAY']\n",
    "val_in = airline_val.drop(['ARRIVAL_DELAY'], axis= 1)\n",
    "preds = lgbm_reg.predict(val_in)\n",
    "mse = mean_squared_error(preds, y_val_airline)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:51:06.760851Z",
     "iopub.status.busy": "2021-12-17T11:51:06.760556Z",
     "iopub.status.idle": "2021-12-17T11:51:07.811129Z",
     "shell.execute_reply": "2021-12-17T11:51:07.810149Z",
     "shell.execute_reply.started": "2021-12-17T11:51:06.760813Z"
    }
   },
   "outputs": [],
   "source": [
    "test_path = '../input/aml-project/test_emiel_v3.csv'\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "test = test.sort_values(by = ['id'])\n",
    "test = test.drop(['id'], axis= 1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:51:17.832289Z",
     "iopub.status.busy": "2021-12-17T11:51:17.831757Z",
     "iopub.status.idle": "2021-12-17T11:51:20.985393Z",
     "shell.execute_reply": "2021-12-17T11:51:20.984553Z",
     "shell.execute_reply.started": "2021-12-17T11:51:17.832256Z"
    }
   },
   "outputs": [],
   "source": [
    "DUMMIES = True\n",
    "X_test = test\n",
    "X_test_cols = list(X_test.columns)\n",
    "X_test_cols.remove('AIRLINE')\n",
    "X_test[X_test_cols] = preprocessing.StandardScaler().fit_transform(X_test[X_test_cols])\n",
    "if DUMMIES:\n",
    "    X_test = pd.get_dummies(X_test, columns = ['AIRLINE'])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:51:26.46638Z",
     "iopub.status.busy": "2021-12-17T11:51:26.465535Z",
     "iopub.status.idle": "2021-12-17T11:51:35.325238Z",
     "shell.execute_reply": "2021-12-17T11:51:35.32449Z",
     "shell.execute_reply.started": "2021-12-17T11:51:26.466338Z"
    }
   },
   "outputs": [],
   "source": [
    "#X_test['AIRLINE_US'] = 0\n",
    "preds = lgbm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:36:42.452731Z",
     "iopub.status.busy": "2021-12-17T11:36:42.45214Z",
     "iopub.status.idle": "2021-12-17T11:36:51.475197Z",
     "shell.execute_reply": "2021-12-17T11:36:51.474485Z",
     "shell.execute_reply.started": "2021-12-17T11:36:42.45269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate prediction results per airline\n",
    "for airline in airline_list:\n",
    "    air_str = 'AIRLINE_' + airline\n",
    "    airline_df = airline_val[airline_val[air_str] == 1]\n",
    "    y_val_airline = airline_df['ARRIVAL_DELAY']\n",
    "    X_val_airline = airline_df.drop(['ARRIVAL_DELAY'], axis = 1)\n",
    "    y_hat = lgbm_reg.predict(X_val_airline)\n",
    "    mse = mean_squared_error(y_val_airline, y_hat)\n",
    "                \n",
    "    print('For all_combined and airline {}, with params: {}, mse is: {}'.format(airline,param_str, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LGBM predicion on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:50:07.634226Z",
     "iopub.status.busy": "2021-12-17T11:50:07.633933Z",
     "iopub.status.idle": "2021-12-17T11:50:08.782533Z",
     "shell.execute_reply": "2021-12-17T11:50:08.781647Z",
     "shell.execute_reply.started": "2021-12-17T11:50:07.634194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test = pd.read_csv('../input/aml-project/test_emiel_v3.csv')\n",
    "test = test.sort_values(by =['id'])\n",
    "X_test= test.drop(['id'], axis = 1)\n",
    "X_test = pd.get_dummies(X_test, columns = ['AIRLINE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T21:37:43.354248Z",
     "iopub.status.busy": "2021-12-16T21:37:43.353793Z",
     "iopub.status.idle": "2021-12-16T21:37:43.60591Z",
     "shell.execute_reply": "2021-12-16T21:37:43.605051Z",
     "shell.execute_reply.started": "2021-12-16T21:37:43.354215Z"
    }
   },
   "outputs": [],
   "source": [
    "train_airlines = train['AIRLINE'].unique()\n",
    "test_airlines = test['AIRLINE'].unique()\n",
    "missing = set(train_airlines) - set(test_airlines)\n",
    "print(missing)\n",
    "X_test['AIRLINE_' + list(missing)[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:50:16.196101Z",
     "iopub.status.busy": "2021-12-17T11:50:16.195823Z",
     "iopub.status.idle": "2021-12-17T11:50:21.749087Z",
     "shell.execute_reply": "2021-12-17T11:50:21.748302Z",
     "shell.execute_reply.started": "2021-12-17T11:50:16.196072Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = lgbm_reg.predict(X_test)\n",
    "print(len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:51:56.468943Z",
     "iopub.status.busy": "2021-12-17T11:51:56.468257Z",
     "iopub.status.idle": "2021-12-17T11:51:56.489786Z",
     "shell.execute_reply": "2021-12-17T11:51:56.488713Z",
     "shell.execute_reply.started": "2021-12-17T11:51:56.468891Z"
    }
   },
   "outputs": [],
   "source": [
    "id_list = np.arange(len(preds))\n",
    "submission_df = pd.DataFrame({\n",
    "                            'id':id_list,\n",
    "                            'ARRIVAL_DELAY' : preds\n",
    "                            })\n",
    "submission_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:52:07.788273Z",
     "iopub.status.busy": "2021-12-17T11:52:07.787965Z",
     "iopub.status.idle": "2021-12-17T11:52:09.687619Z",
     "shell.execute_reply": "2021-12-17T11:52:09.686857Z",
     "shell.execute_reply.started": "2021-12-17T11:52:07.788238Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:33:43.431054Z",
     "iopub.status.busy": "2021-12-17T11:33:43.430271Z",
     "iopub.status.idle": "2021-12-17T11:33:43.625061Z",
     "shell.execute_reply": "2021-12-17T11:33:43.623534Z",
     "shell.execute_reply.started": "2021-12-17T11:33:43.431006Z"
    }
   },
   "outputs": [],
   "source": [
    "subsamp = pd.read_csv('../input/flight-delays-prediction-challeng2021/submit_sample.csv')\n",
    "subsamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-17T11:33:21.963267Z",
     "iopub.status.busy": "2021-12-17T11:33:21.962635Z",
     "iopub.status.idle": "2021-12-17T11:33:22.146624Z",
     "shell.execute_reply": "2021-12-17T11:33:22.145981Z",
     "shell.execute_reply.started": "2021-12-17T11:33:21.963231Z"
    }
   },
   "outputs": [],
   "source": [
    "subdf = pd.read_csv('./submission.csv')\n",
    "subdf.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T21:44:19.851977Z",
     "iopub.status.busy": "2021-12-16T21:44:19.851694Z",
     "iopub.status.idle": "2021-12-16T21:44:21.060209Z",
     "shell.execute_reply": "2021-12-16T21:44:21.059313Z",
     "shell.execute_reply.started": "2021-12-16T21:44:19.851949Z"
    }
   },
   "outputs": [],
   "source": [
    "lgbm_reg.booster_.save_model('LGB_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T11:08:47.729272Z",
     "iopub.status.busy": "2021-12-15T11:08:47.729013Z",
     "iopub.status.idle": "2021-12-15T11:08:52.786732Z",
     "shell.execute_reply": "2021-12-15T11:08:52.785794Z",
     "shell.execute_reply.started": "2021-12-15T11:08:47.729241Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_val = val.drop(['ARRIVAL_DELAY'], axis=1).values\n",
    "y_val = val['ARRIVAL_DELAY'].values\n",
    "y_hat = clf.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_hat)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:00:08.151468Z",
     "iopub.status.busy": "2021-12-12T16:00:08.150727Z",
     "iopub.status.idle": "2021-12-12T16:00:08.157557Z",
     "shell.execute_reply": "2021-12-12T16:00:08.156859Z",
     "shell.execute_reply.started": "2021-12-12T16:00:08.151428Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:21:19.930689Z",
     "iopub.status.busy": "2021-12-12T16:21:19.930291Z",
     "iopub.status.idle": "2021-12-12T16:21:19.949465Z",
     "shell.execute_reply": "2021-12-12T16:21:19.948362Z",
     "shell.execute_reply.started": "2021-12-12T16:21:19.930598Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "      #create simple neural network with 6 layers of which the number of nodes can be changed\n",
    "      def __init__(self, inshape, l1,l2,l3,l4,l5,l6):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(inshape, l1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l1, l2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l2, l3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l3, l4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l4, l5),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l5, l6),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l6, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "      def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:21:20.294626Z",
     "iopub.status.busy": "2021-12-12T16:21:20.294342Z",
     "iopub.status.idle": "2021-12-12T16:21:20.306365Z",
     "shell.execute_reply": "2021-12-12T16:21:20.305637Z",
     "shell.execute_reply.started": "2021-12-12T16:21:20.29458Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, batch_size =  16, num_workers = 2,in_vars = new_vars, target= 'ARRIVAL_DELAY'):\n",
    "    '''create pytorch dataloader with df, feature columns and target column as input '''\n",
    "    train_subset, val_subset = train_test_split(df, test_size = 0.2)\n",
    "    train_subset = train_subset.reset_index(drop=True)\n",
    "    val_subset = val_subset.reset_index(drop=True)\n",
    "    \n",
    "    train_input = torch.tensor(train_subset[in_vars].values.astype(np.float32))\n",
    "    train_target = torch.tensor(train_subset[target].values.astype(np.float32))\n",
    "    \n",
    "    val_input = torch.tensor(val_subset[in_vars].values.astype(np.float32))\n",
    "    val_target = torch.tensor(val_subset[target].values.astype(np.float32))\n",
    "    \n",
    "    train_tensor = data_utils.TensorDataset(train_input, train_target) \n",
    "    val_tensor = data_utils.TensorDataset(val_input, val_target) \n",
    "    \n",
    "    train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "    val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:22:03.796386Z",
     "iopub.status.busy": "2021-12-12T16:22:03.793393Z",
     "iopub.status.idle": "2021-12-12T16:22:03.82584Z",
     "shell.execute_reply": "2021-12-12T16:22:03.824835Z",
     "shell.execute_reply.started": "2021-12-12T16:22:03.79634Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cifar(config, inshape = len(new_vars),train_data = None, checkpoint_dir=None):\n",
    "    # train pytorch neural network\n",
    "    net = Net(inshape,l1 =config[\"l1\"], l2 =config[\"l2\"],\n",
    "             l3 =config[\"l3\"], l4 =config[\"l4\"],\n",
    "             l5 =config[\"l5\"], l6 =config[\"l6\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adadelta(net.parameters(), lr = config['lr'])\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainloader, valloader = load_data(train_data, batch_size = int(config[\"batch_size\"]))\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for data in trainloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels[:,None])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            #if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            #    print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "            #                                    running_loss / epoch_steps))\n",
    "            #    running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for data in valloader:\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                \n",
    "                loss = mean_squared_error(outputs.cpu().numpy(), labels.cpu().numpy())\n",
    "                val_loss += loss\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps))\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:35:06.279911Z",
     "iopub.status.busy": "2021-12-12T16:35:06.279469Z",
     "iopub.status.idle": "2021-12-12T16:35:06.293938Z",
     "shell.execute_reply": "2021-12-12T16:35:06.292291Z",
     "shell.execute_reply.started": "2021-12-12T16:35:06.27985Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
    "    config = {\n",
    "        \"l1\": tune.grid_search([1600,1400]),\n",
    "        \"l2\": tune.grid_search([1200,1000]),\n",
    "        \"l3\": tune.grid_search([800,600]),\n",
    "        \"l4\": tune.grid_search([400,300]),\n",
    "        \"l5\": tune.grid_search([200,100]),\n",
    "        \"l6\": tune.grid_search([50,25]),\n",
    "        \"lr\": tune.grid_search([1e-3,1e-4]),\n",
    "        \"batch_size\": tune.grid_search([16,32,64])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train_cifar, train_data = ftrain.sample(2000)),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        verbose = 1)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    # You can change the number of GPUs per trial here:\n",
    "#    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:35:06.451544Z",
     "iopub.status.busy": "2021-12-12T16:35:06.450779Z",
     "iopub.status.idle": "2021-12-12T16:35:15.056869Z",
     "shell.execute_reply": "2021-12-12T16:35:15.055399Z",
     "shell.execute_reply.started": "2021-12-12T16:35:06.451501Z"
    }
   },
   "outputs": [],
   "source": [
    "main(num_samples=1, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "      def __init__(self, inshape, l1,l2,l3,l4,l5,l6):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(inshape, l1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l1, l2),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l2, l3),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l3, l4),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l4, l5),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l5, l6),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(l6, 1)\n",
    "        )\n",
    "\n",
    "      def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "# To load the model we create the net and then load it in paralel since that was how it was trained\n",
    "anet = Net(11352, 1600, 1200, 800, 300,200,50)\n",
    "anet = nn.DataParallel(anet)\n",
    "anet.load_state_dict(torch.load('../input/aml-model/best_model_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(infile)\n",
    "train_oh = pd.get_dummies(train_data, columns = cat_cols)\n",
    "new_vars = list(train_oh.columns[8:15]) + list(train_oh.columns[16:])\n",
    "input_length = len(new_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T14:44:16.865688Z",
     "iopub.status.busy": "2021-12-10T14:44:16.865396Z",
     "iopub.status.idle": "2021-12-10T14:44:17.997789Z",
     "shell.execute_reply": "2021-12-10T14:44:17.996956Z",
     "shell.execute_reply.started": "2021-12-10T14:44:16.865658Z"
    }
   },
   "outputs": [],
   "source": [
    "submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\n",
    "submit_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T16:35:53.51018Z",
     "iopub.status.busy": "2021-12-10T16:35:53.509448Z",
     "iopub.status.idle": "2021-12-10T16:35:55.619966Z",
     "shell.execute_reply": "2021-12-10T16:35:55.619056Z",
     "shell.execute_reply.started": "2021-12-10T16:35:53.510133Z"
    }
   },
   "outputs": [],
   "source": [
    "submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\n",
    "print(len(submit_sample))\n",
    "airports_origin = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'ORIGIN_AIRPORT'})\n",
    "airports_arrive = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'DESTINATION_AIRPORT'})\n",
    "submit_sample1 = submit_sample.merge(airports_origin, on = 'ORIGIN_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_origin', 'LONGITUDE' : 'LONGITUDE_origin'})\n",
    "submit_sample2 = submit_sample1.merge(airports_arrive, on = 'DESTINATION_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_arrival', 'LONGITUDE' : 'LONGITUDE_arrival'})\n",
    "print(len(submit_sample2.dropna()))\n",
    "submit_sample2[cont_cols] = s_scaler.transform(submit_sample2[cont_cols])\n",
    "submit_sample2 = pd.get_dummies(submit_sample2, columns = cat_cols)\n",
    "submit_sample2 = submit_sample2.sort_values(by=['id'])\n",
    "submit_sample2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T20:04:36.023546Z",
     "iopub.status.busy": "2021-12-16T20:04:36.023035Z",
     "iopub.status.idle": "2021-12-16T20:04:37.752466Z",
     "shell.execute_reply": "2021-12-16T20:04:37.751708Z",
     "shell.execute_reply.started": "2021-12-16T20:04:36.023508Z"
    }
   },
   "outputs": [],
   "source": [
    "submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\n",
    "print(len(submit_sample))\n",
    "airports_origin = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'ORIGIN_AIRPORT'})\n",
    "airports_arrive = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'DESTINATION_AIRPORT'})\n",
    "submit_sample1 = submit_sample.merge(airports_origin, on = 'ORIGIN_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_origin', 'LONGITUDE' : 'LONGITUDE_origin'})\n",
    "submit_sample2 = submit_sample1.merge(airports_arrive, on = 'DESTINATION_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_arrival', 'LONGITUDE' : 'LONGITUDE_arrival'})\n",
    "print(len(submit_sample2.dropna()))\n",
    "print(submit_sample2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T20:08:18.795067Z",
     "iopub.status.busy": "2021-12-16T20:08:18.79449Z",
     "iopub.status.idle": "2021-12-16T20:08:18.79964Z",
     "shell.execute_reply": "2021-12-16T20:08:18.798686Z",
     "shell.execute_reply.started": "2021-12-16T20:08:18.795028Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_cols = submit_sample2.columns[10:]\n",
    "cont_cols = submit_sample2.columns[10:]\n",
    "submit_sample2[cont_cols] = s_scaler.transform(submit_sample2[cont_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First test on small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T20:16:48.706788Z",
     "iopub.status.busy": "2021-12-16T20:16:48.706231Z",
     "iopub.status.idle": "2021-12-16T20:16:48.851718Z",
     "shell.execute_reply": "2021-12-16T20:16:48.850973Z",
     "shell.execute_reply.started": "2021-12-16T20:16:48.706747Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_samp = submit_sample2.sample(50)\n",
    "in_list = []\n",
    "for i, row in sub_samp.iterrows():\n",
    "    cont_vals = list(row[cont_cols])\n",
    "    for cat in oh_dict:\n",
    "        cat_val = row[cat]\n",
    "        cat_un = list(oh_dict[cat])\n",
    "        cat_zeros = np.zeros(len(cat_un))\n",
    "        cat_ind = cat_un.index(cat_val)\n",
    "        cat_zeros[cat_ind] = 1\n",
    "        cont_vals += list(cat_zeros)\n",
    "    in_list.append(cont_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-16T20:18:10.658378Z",
     "iopub.status.busy": "2021-12-16T20:18:10.658123Z",
     "iopub.status.idle": "2021-12-16T20:18:10.663002Z",
     "shell.execute_reply": "2021-12-16T20:18:10.662259Z",
     "shell.execute_reply.started": "2021-12-16T20:18:10.658349Z"
    }
   },
   "outputs": [],
   "source": [
    "in_tensor = torch.tensor(in_list)\n",
    "preds = net.predict(in_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:  Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from tensorflow.keras.layers import Dense, Conv2D , SeparableConv2D, MaxPool2D, Flatten , Dropout , BatchNormalization\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from bayes_opt import BayesianOptimization\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T15:11:21.070133Z",
     "iopub.status.busy": "2021-12-10T15:11:21.069861Z",
     "iopub.status.idle": "2021-12-10T15:11:23.580061Z",
     "shell.execute_reply": "2021-12-10T15:11:23.579364Z",
     "shell.execute_reply.started": "2021-12-10T15:11:21.070103Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, input_shape=input_shape, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(450, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(300, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-10T16:00:29.528774Z",
     "iopub.status.busy": "2021-12-10T16:00:29.528135Z",
     "iopub.status.idle": "2021-12-10T16:33:54.933703Z",
     "shell.execute_reply": "2021-12-10T16:33:54.932994Z",
     "shell.execute_reply.started": "2021-12-10T16:00:29.528728Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(ftrain[new_vars], ftrain['ARRIVAL_DELAY'], epochs = 10, batch_size = 32, validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T18:04:34.401785Z",
     "iopub.status.busy": "2021-12-11T18:04:34.401446Z",
     "iopub.status.idle": "2021-12-11T18:54:52.620268Z",
     "shell.execute_reply": "2021-12-11T18:54:52.618685Z",
     "shell.execute_reply.started": "2021-12-11T18:04:34.40175Z"
    }
   },
   "outputs": [],
   "source": [
    "all_cat_cols = ['DAY','DAY_OF_WEEK','AIRLINE', 'FLIGHT_NUMBER','TAIL_NUMBER']\n",
    "for j in range(3):\n",
    "    cat_cols = all_cat_cols[:3+j]\n",
    "    print(cat_cols)\n",
    "    dir_name = 'cat_col' + str(j)\n",
    "\n",
    "    train_oh = pd.get_dummies(small_train, columns = cat_cols)\n",
    "\n",
    "    new_vars = list(train_oh.columns[7:14]) + list(train_oh.columns[16:])\n",
    "    input_length = len(new_vars)\n",
    "    input_shape = (input_length,)\n",
    "    \n",
    "    def model_builder(hp, j= j,input_shape = input_shape):\n",
    "        model = keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "\n",
    "        for i in range(6):\n",
    "            model.add(tf.keras.layers.Dense(units=hp.Choice('units_' + str(i), \n",
    "                                                            values=[1200,1000,800,700,\n",
    "                                                                    600,500,400,300,\n",
    "                                                                    200,100,50,25][i*2:i*2+2]),\n",
    "                                            activation='relu'))\n",
    "\n",
    "        model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "        model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=[\n",
    "                        'MeanSquaredError'\n",
    "                        ]\n",
    "                     )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mean_squared_error',\n",
    "                     max_epochs=60,\n",
    "                     factor=3,\n",
    "                     directory=dir_name ,\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "    \n",
    "    tuner.search(train_oh[new_vars], \n",
    "             train_oh['ARRIVAL_DELAY'], \n",
    "             epochs=60, \n",
    "             validation_split=0.15,\n",
    "             callbacks = [stop_early])\n",
    "\n",
    "    # Get the optimal hyperparameters\n",
    "    best_hp=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    model = tuner.hypermodel.build(best_hp)\n",
    "    model_name = 'best_model_' + str(j)\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T15:45:18.173982Z",
     "iopub.status.busy": "2021-12-08T15:45:18.173513Z",
     "iopub.status.idle": "2021-12-08T15:45:18.182885Z",
     "shell.execute_reply": "2021-12-08T15:45:18.182018Z",
     "shell.execute_reply.started": "2021-12-08T15:45:18.173943Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_builder(hp, input_shape = input_shape):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "    \n",
    "    for i in range(6):\n",
    "        model.add(tf.keras.layers.Dense(units=hp.Choice('units_' + str(i), \n",
    "                                                        values=[600,400,300,200,100,50,25][i:i+2]),\n",
    "                                        activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=hp_learning_rate),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=[\n",
    "                    'MeanSquaredError'\n",
    "                    ],\n",
    "            steps_per_execution=8\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T15:45:18.185138Z",
     "iopub.status.busy": "2021-12-08T15:45:18.184871Z",
     "iopub.status.idle": "2021-12-08T15:45:20.763903Z",
     "shell.execute_reply": "2021-12-08T15:45:20.763082Z",
     "shell.execute_reply.started": "2021-12-08T15:45:18.185103Z"
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_mean_squared_error',\n",
    "                     max_epochs=50,\n",
    "                     factor=3,\n",
    "                     directory=dir_name ,\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T15:45:29.252533Z",
     "iopub.status.busy": "2021-12-08T15:45:29.252266Z"
    }
   },
   "outputs": [],
   "source": [
    "tuner.search(train[new_vars], \n",
    "             train['ARRIVAL_DELAY'], \n",
    "             epochs=50, \n",
    "             validation_split=0.15,\n",
    "             callbacks = [stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-08T11:35:22.760684Z",
     "iopub.status.busy": "2021-12-08T11:35:22.76004Z",
     "iopub.status.idle": "2021-12-08T11:35:22.773542Z",
     "shell.execute_reply": "2021-12-08T11:35:22.772642Z",
     "shell.execute_reply.started": "2021-12-08T11:35:22.760638Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model(train_data, val_data, input_size = input_shape, input_columns = new_vars):\n",
    "    \n",
    "    def dense_training(train_df, val_df, list_of_params, input_size, input_columns, \n",
    "                       epochs, remove_outliers, batch_size, target_column = 'ARRIVAL_DELAY'):\n",
    "        \n",
    "        if remove_outliers:\n",
    "            z_scores = zscore(train_df[target_column])\n",
    "            abs_z_scores = np.abs(z_scores)\n",
    "            filtered_entries = (abs_z_scores < 3)\n",
    "            train_df = train_df[filtered_entries]\n",
    "        \n",
    "        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "        \n",
    "        model = build_model(list_of_params, input_size)\n",
    "        result = model.fit(train_df[input_columns], train_df[target_column], \n",
    "                        epochs=epochs, batch_size=batch_size, \n",
    "                        callbacks=[stop_early],\n",
    "                        validation_split=(val_df[input_columns],val_df[target_column]))\n",
    "        return result.history['val_loss']\n",
    "    \n",
    "                           \n",
    "    optimizer = BayesianOptimization(\n",
    "    f=dense_training,\n",
    "    pbounds={\n",
    "        'train_df': train_data,\n",
    "        'val_df' : val_data,\n",
    "        'input_size' : input_size,\n",
    "        'input_columns' : input_columns,\n",
    "        'remove outliers' : [True, False],\n",
    "        'list_of_params': [[200,100,50,25],[400,200,100,50,25],[800,400,200,100,50,25],[600,450,300,150,75,25]], \n",
    "        'epochs': (10,20,50,80,100),\n",
    "        'batch_size': (16,32,64)\n",
    "            },\n",
    "    random_state = 12,\n",
    "    verbose=2\n",
    "    )\n",
    "                           \n",
    "    optimizer.maximize(n_iter=10)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective=\"val_accuracy\",\n",
    "                     max_epochs=100,\n",
    "                     factor=3,\n",
    "                     hyperband_iterations=10,\n",
    "                     directory=\"kt_dir\",\n",
    "                     project_name=\"kt_hyperband\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
