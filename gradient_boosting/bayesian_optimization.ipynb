{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d97b6c-b8bc-4d7a-91f0-fb8aea097748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic tools \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#tuning hyperparameters\n",
    "from bayes_opt import BayesianOptimization  \n",
    "\n",
    "#building models\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88874857-4a29-4a67-93ab-89729335dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    function to reduce pandas dataframe memory usage\n",
    "    \"\"\"\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a3a75-7f95-4482-904e-52c9ef1b4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, and test set\n",
    "df_train_validation = pd.read_csv(\"final_train_val.csv\", low_memory=False, index_col=\"id\")\n",
    "df_test = pd.read_csv(\"final_test.csv\", low_memory=False, index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714664e0-b920-445e-95f8-1ce3afa30e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce memory of the loaded dataframes\n",
    "df_train_validation = reduce_mem_usage(df_train_validation) \n",
    "df_test = reduce_mem_usage(df_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267c52f-7f85-4112-b922-d417fedb56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for LGBM\n",
    "df_train, df_validation = train_test_split(df_train_validation, test_size=0.20, random_state = 42)\n",
    "X_train, y_train = df_train.drop(\"ARRIVAL_DELAY\", axis=1), df_train[\"ARRIVAL_DELAY\"]\n",
    "X_val, y_val = df_validation.drop(\"ARRIVAL_DELAY\", axis=1), df_validation[\"ARRIVAL_DELAY\"]\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e4dc2-01c6-48cb-8173-70c97431b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgbm(X, y, init_round=15, opt_round=25, n_folds=10, random_seed=6, output_process=False):\n",
    "    \"\"\"\n",
    "    Sets up a Bayesian optimization schedule using BayesianOptimization and returns optimial parameters found\n",
    "    :params:\n",
    "    init_round: Number of initial random evaluations to get an idea of the problem space\n",
    "    opt_round: Number of Bayesian optimization steps\n",
    "    n_folds: Number of folds for the LGBM model cross-validation during optimization\n",
    "    random_seed: Random state for reproducibility\n",
    "    \"\"\"\n",
    "    # prepare data\n",
    "    train_data = lgbm.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgbm_eval(learning_rate, num_iterations, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf, min_sum_hessian_in_leaf):\n",
    "        \"\"\"\n",
    "        Fits an LGBM model and returns the L2 (MSE) error resulting from the cross-validation\n",
    "        :params: are the hyper-parameters to be optimized\n",
    "        \"\"\"\n",
    "        params = {'application':'regression_l2', 'metric':'mse', 'early_stopping_round': 3, 'verbosity': -1}\n",
    "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['num_leaves'] = int(round(2**max_depth))\n",
    "        params['num_iterations'] = int(round(num_iterations))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_bin'] = int(round(max_depth))\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
    "        \n",
    "        cv_result = lgbm.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, metrics=['l2'])\n",
    "        # Return negative MSE to be maximized\n",
    "        return -max(cv_result['l2-mean'])\n",
    "     \n",
    "    lgbmBO = BayesianOptimization(lgbm_eval, {\n",
    "        'learning_rate': (0.1, 0.5),\n",
    "        'num_iterations': (10, 400),\n",
    "        'feature_fraction': (0.01, 1.0),\n",
    "        'bagging_fraction': (0.01, 1.0),\n",
    "        'max_depth': (8, 15),\n",
    "        'max_bin':(10,200),\n",
    "        'min_data_in_leaf': (10, 400),\n",
    "        'min_sum_hessian_in_leaf':(0,400),\n",
    "    }, random_state=random_seed)\n",
    "\n",
    "    lgbmBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    model_mse=[]\n",
    "    for model in range(len(lgbmBO.res)):\n",
    "        model_mse.append(lgbmBO.res[model]['target'])\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbmBO.res[pd.Series(model_mse).idxmax()]['target'],lgbmBO.res[pd.Series(model_mse).idxmax()]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45991e05-cc2f-44b7-9249-a4ea85f3c585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exectute the Bayesian optimization and print the best parameters found\n",
    "opt_params = bayes_parameter_opt_lgbm(X_train, y_train, init_round=50, opt_round=50, n_folds=10, random_seed=6)\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000920be-c798-4c4b-baec-c50761eb7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09cd615-a4b7-41c1-b1d9-e98ce7345872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimial parameters dict as a pickle\n",
    "with open('opt_params.pickle', 'wb') as f:\n",
    "    pickle.dump(opt_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    reg = xgb.XGBRegressor(n_estimators =space['n_estimators'], \n",
    "                           max_depth = int(space['max_depth']),\n",
    "                           gamma = space['gamma'], \n",
    "                           reg_alpha = int(space['reg_alpha']),\n",
    "                           min_child_weight=int(space['min_child_weight']),\n",
    "                           colsample_bytree=int(space['colsample_bytree']))\n",
    "\n",
    "    eval_set  = [(X_train, y_train), (X_val, y_val)]\n",
    "\n",
    "    reg.fit(X_train, y_train, eval_set=eval_set, eval_metric = 'rmse',\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    val_pred = reg.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, val_pred)\n",
    "    return{'loss':mse, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "\n",
    "print(best_hyperparams)\n",
    "\n",
    "print(\"The best hyperparameters are : \",\"\\n\")\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
