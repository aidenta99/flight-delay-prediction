{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PART 1: Random Forest and LGBM","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:40:36.363966Z","iopub.execute_input":"2021-12-17T11:40:36.364828Z","iopub.status.idle":"2021-12-17T11:40:37.368284Z","shell.execute_reply.started":"2021-12-17T11:40:36.364783Z","shell.execute_reply":"2021-12-17T11:40:37.367461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DUMMIES = False\nREMOVE_US = True\n\n# Read data\ntrain = pd.read_csv('../input/aml-project/train_emiel_v3.csv')\nX_train = train.drop(['id'], axis = 1)\nX_train_columns = list(X_train.columns)\nX_train_columns.remove('AIRLINE')\nX_train_columns_no_target = X_train_columns\nX_train_columns_no_target.remove('ARRIVAL_DELAY')\nX_train[X_train_columns_no_target] = preprocessing.StandardScaler().fit_transform(X_train[X_train_columns], y ='ARRIVAL_DELAY')\nif REMOVE_US:\n    X_train = X_train[X_train['AIRLINE'] != 'US']\n\nif DUMMIES:\n    X_train = pd.get_dummies(X_train, columns = ['AIRLINE'])\ny_train = train['ARRIVAL_DELAY']\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:40:37.369683Z","iopub.execute_input":"2021-12-17T11:40:37.369901Z","iopub.status.idle":"2021-12-17T11:41:05.800838Z","shell.execute_reply.started":"2021-12-17T11:40:37.369874Z","shell.execute_reply":"2021-12-17T11:41:05.799895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nX_train = train.dropna().drop(['ARRIVAL_DELAY'], axis=1).values\ny_train = train.dropna()['ARRIVAL_DELAY'].values\nX_val = val.dropna().drop(['ARRIVAL_DELAY'], axis=1).values\ny_val = val.dropna()['ARRIVAL_DELAY'].values\nrfg = RandomForestRegressor(n_estimators = 200, max_depth = 15, min_weight_fraction_leaf = 0.000, verbose = 1)\nrfg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T10:59:31.614323Z","iopub.execute_input":"2021-12-17T10:59:31.615403Z","iopub.status.idle":"2021-12-17T10:59:32.994208Z","shell.execute_reply.started":"2021-12-17T10:59:31.615269Z","shell.execute_reply":"2021-12-17T10:59:32.992437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the lightgbm model\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\n#emiel_csv = val.drop(['ARRIVAL_DELAY'], axis=1).values\n#emiel_csv = val['ARRIVAL_DELAY'].values\n\n\nmd_list = [15]\nmsg_list = [0]\nlr_list = [0.1]\nn_est = [150]\n\n#small manual grid search\nairline_dict ={}\nairline_list = X_train['AIRLINE'].unique()\nfor md in md_list:\n    nl = 2 ** md\n    for msg in msg_list:\n        for lr in lr_list:\n            for n in n_est:\n                for airline in airline_list:\n                    airline_df = X_train[X_train['AIRLINE'] == airline]\n                    airline_df = airline_df.drop(['AIRLINE'], axis = 1)\n                    airline_train, airline_val = train_test_split(airline_df, test_size = 0.15)\n                    \n                    y_train_airline = airline_train['ARRIVAL_DELAY']\n                    X_train_airline = airline_train.drop(['ARRIVAL_DELAY'], axis = 1)\n                    \n                    y_val_airline = airline_val['ARRIVAL_DELAY']\n                    X_val_airline = airline_val.drop(['ARRIVAL_DELAY'], axis = 1)\n                    \n                    lgbm_reg = lgb.LGBMRegressor(num_leaves = nl, min_split_gain =msg, max_depth = md, \n                                                 learning_rate =lr, n_estimators= n)\n                    param_str = 'md {} msg {} lr {} n {}'.format(md,msg,lr,n)\n                    param_list = [md,msg,lr,n]\n                    lgbm_reg.fit(X_train_airline, y_train_airline, eval_metric = 'mse')\n                    \n                    y_hat = lgbm_reg.predict(X_val_airline)\n                    mse = mean_squared_error(y_val_airline, y_hat)\n                    print('For {}, with params: {}, mse is: {}'.format(airline, param_str, mse))\n                    \n                    if mse < 100:\n                        airline_dict[airline] = lgbm_reg\n                \n                \n\n                ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:00:56.326464Z","iopub.execute_input":"2021-12-17T11:00:56.329322Z","iopub.status.idle":"2021-12-17T11:08:18.010037Z","shell.execute_reply.started":"2021-12-17T11:00:56.329236Z","shell.execute_reply":"2021-12-17T11:08:18.005167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the lightgbm model\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfull_train_data = pd.get_dummies(X_train, columns = ['AIRLINE'])\nairline_train, airline_val = train_test_split(full_train_data, test_size = 0.15)\n\ny_train_airline = full_train_data['ARRIVAL_DELAY']\nX_train_airline = full_train_data.drop(['ARRIVAL_DELAY'], axis = 1)\n\n\n#fit the model\nlgbm_reg = lgb.LGBMRegressor(num_leaves = 2**15, min_split_gain =0, max_depth = 15, \n                             learning_rate =0.1, n_estimators= 150)\n\nlgbm_reg.fit(X_train_airline, y_train_airline, eval_metric = 'mse')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:43:20.43334Z","iopub.execute_input":"2021-12-17T11:43:20.433969Z","iopub.status.idle":"2021-12-17T11:45:21.733424Z","shell.execute_reply.started":"2021-12-17T11:43:20.433919Z","shell.execute_reply":"2021-12-17T11:45:21.732261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_airline = airline_val['ARRIVAL_DELAY']\nval_in = airline_val.drop(['ARRIVAL_DELAY'], axis= 1)\npreds = lgbm_reg.predict(val_in)\nmse = mean_squared_error(preds, y_val_airline)\nprint(mse)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:45:59.648147Z","iopub.execute_input":"2021-12-17T11:45:59.649069Z","iopub.status.idle":"2021-12-17T11:46:07.102655Z","shell.execute_reply.started":"2021-12-17T11:45:59.649017Z","shell.execute_reply":"2021-12-17T11:46:07.101981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '../input/aml-project/test_emiel_v3.csv'\n\ntest = pd.read_csv(test_path)\ntest = test.sort_values(by = ['id'])\ntest = test.drop(['id'], axis= 1)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:51:06.760556Z","iopub.execute_input":"2021-12-17T11:51:06.760851Z","iopub.status.idle":"2021-12-17T11:51:07.811129Z","shell.execute_reply.started":"2021-12-17T11:51:06.760813Z","shell.execute_reply":"2021-12-17T11:51:07.810149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DUMMIES = True\nX_test = test\nX_test_cols = list(X_test.columns)\nX_test_cols.remove('AIRLINE')\nX_test[X_test_cols] = preprocessing.StandardScaler().fit_transform(X_test[X_test_cols])\nif DUMMIES:\n    X_test = pd.get_dummies(X_test, columns = ['AIRLINE'])\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:51:17.831757Z","iopub.execute_input":"2021-12-17T11:51:17.832289Z","iopub.status.idle":"2021-12-17T11:51:20.985393Z","shell.execute_reply.started":"2021-12-17T11:51:17.832256Z","shell.execute_reply":"2021-12-17T11:51:20.984553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_test['AIRLINE_US'] = 0\npreds = lgbm_reg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:51:26.465535Z","iopub.execute_input":"2021-12-17T11:51:26.46638Z","iopub.status.idle":"2021-12-17T11:51:35.325238Z","shell.execute_reply.started":"2021-12-17T11:51:26.466338Z","shell.execute_reply":"2021-12-17T11:51:35.32449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate prediction results per airline\nfor airline in airline_list:\n    air_str = 'AIRLINE_' + airline\n    airline_df = airline_val[airline_val[air_str] == 1]\n    y_val_airline = airline_df['ARRIVAL_DELAY']\n    X_val_airline = airline_df.drop(['ARRIVAL_DELAY'], axis = 1)\n    y_hat = lgbm_reg.predict(X_val_airline)\n    mse = mean_squared_error(y_val_airline, y_hat)\n                \n    print('For all_combined and airline {}, with params: {}, mse is: {}'.format(airline,param_str, mse))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:36:42.45214Z","iopub.execute_input":"2021-12-17T11:36:42.452731Z","iopub.status.idle":"2021-12-17T11:36:51.475197Z","shell.execute_reply.started":"2021-12-17T11:36:42.45269Z","shell.execute_reply":"2021-12-17T11:36:51.474485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on test set\ntest = pd.read_csv('../input/aml-project/test_emiel_v3.csv')\ntest = test.sort_values(by =['id'])\nX_test= test.drop(['id'], axis = 1)\nX_test = pd.get_dummies(X_test, columns = ['AIRLINE'])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:50:07.633933Z","iopub.execute_input":"2021-12-17T11:50:07.634226Z","iopub.status.idle":"2021-12-17T11:50:08.782533Z","shell.execute_reply.started":"2021-12-17T11:50:07.634194Z","shell.execute_reply":"2021-12-17T11:50:08.781647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_airlines = train['AIRLINE'].unique()\ntest_airlines = test['AIRLINE'].unique()\nmissing = set(train_airlines) - set(test_airlines)\nprint(missing)\nX_test['AIRLINE_' + list(missing)[0]] = 0","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:37:43.353793Z","iopub.execute_input":"2021-12-16T21:37:43.354248Z","iopub.status.idle":"2021-12-16T21:37:43.60591Z","shell.execute_reply.started":"2021-12-16T21:37:43.354215Z","shell.execute_reply":"2021-12-16T21:37:43.605051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = lgbm_reg.predict(X_test)\nprint(len(y_hat))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:50:16.195823Z","iopub.execute_input":"2021-12-17T11:50:16.196101Z","iopub.status.idle":"2021-12-17T11:50:21.749087Z","shell.execute_reply.started":"2021-12-17T11:50:16.196072Z","shell.execute_reply":"2021-12-17T11:50:21.748302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_list = np.arange(len(preds))\nsubmission_df = pd.DataFrame({\n                            'id':id_list,\n                            'ARRIVAL_DELAY' : preds\n                            })\nsubmission_df.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:51:56.468257Z","iopub.execute_input":"2021-12-17T11:51:56.468943Z","iopub.status.idle":"2021-12-17T11:51:56.489786Z","shell.execute_reply.started":"2021-12-17T11:51:56.468891Z","shell.execute_reply":"2021-12-17T11:51:56.488713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index= False)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:52:07.787965Z","iopub.execute_input":"2021-12-17T11:52:07.788273Z","iopub.status.idle":"2021-12-17T11:52:09.687619Z","shell.execute_reply.started":"2021-12-17T11:52:07.788238Z","shell.execute_reply":"2021-12-17T11:52:09.686857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsamp = pd.read_csv('../input/flight-delays-prediction-challeng2021/submit_sample.csv')\nsubsamp.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:33:43.430271Z","iopub.execute_input":"2021-12-17T11:33:43.431054Z","iopub.status.idle":"2021-12-17T11:33:43.625061Z","shell.execute_reply.started":"2021-12-17T11:33:43.431006Z","shell.execute_reply":"2021-12-17T11:33:43.623534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subdf = pd.read_csv('./submission.csv')\nsubdf.head(17)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T11:33:21.962635Z","iopub.execute_input":"2021-12-17T11:33:21.963267Z","iopub.status.idle":"2021-12-17T11:33:22.146624Z","shell.execute_reply.started":"2021-12-17T11:33:21.963231Z","shell.execute_reply":"2021-12-17T11:33:22.145981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_reg.booster_.save_model('LGB_model.txt')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:44:19.851694Z","iopub.execute_input":"2021-12-16T21:44:19.851977Z","iopub.status.idle":"2021-12-16T21:44:21.060209Z","shell.execute_reply.started":"2021-12-16T21:44:19.851949Z","shell.execute_reply":"2021-12-16T21:44:21.059313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nX_val = val.drop(['ARRIVAL_DELAY'], axis=1).values\ny_val = val['ARRIVAL_DELAY'].values\ny_hat = clf.predict(X_val)\nmse = mean_squared_error(y_val, y_hat)\nprint(mse)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T11:08:47.729013Z","iopub.execute_input":"2021-12-15T11:08:47.729272Z","iopub.status.idle":"2021-12-15T11:08:52.786732Z","shell.execute_reply.started":"2021-12-15T11:08:47.729241Z","shell.execute_reply":"2021-12-15T11:08:52.785794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch","metadata":{}},{"cell_type":"code","source":"from functools import partial\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler\nfrom sklearn.model_selection import train_test_split\nimport torch.utils.data as data_utils\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:00:08.150727Z","iopub.execute_input":"2021-12-12T16:00:08.151468Z","iopub.status.idle":"2021-12-12T16:00:08.157557Z","shell.execute_reply.started":"2021-12-12T16:00:08.151428Z","shell.execute_reply":"2021-12-12T16:00:08.156859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net2(nn.Module):\n    def __init__(self, inshape, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n\n        self.fc1=nn.Linear(inshape, l1),\n        nn.ReLU(),\n        self.fc2=nn.Linear(l1, l2),\n        nn.ReLU(),\n        self.fc3=nn.Linear(l1, 1),\n\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-12T15:46:31.756728Z","iopub.execute_input":"2021-12-12T15:46:31.757551Z","iopub.status.idle":"2021-12-12T15:46:31.766883Z","shell.execute_reply.started":"2021-12-12T15:46:31.757501Z","shell.execute_reply":"2021-12-12T15:46:31.765869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n      #create simple neural network with 6 layers\n      def __init__(self, inshape, l1,l2,l3,l4,l5,l6):\n        super().__init__()\n        self.layers = nn.Sequential(\n          nn.Flatten(),\n          nn.Linear(inshape, l1),\n          nn.ReLU(),\n          nn.Linear(l1, l2),\n          nn.ReLU(),\n          nn.Linear(l2, l3),\n          nn.ReLU(),\n          nn.Linear(l3, l4),\n          nn.ReLU(),\n          nn.Linear(l4, l5),\n          nn.ReLU(),\n          nn.Linear(l5, l6),\n          nn.ReLU(),\n          nn.Linear(l6, 1)\n        )\n\n\n      def forward(self, x):\n        #x = self.flatten(x)\n        '''\n          Forward pass\n        '''\n        return self.layers(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:21:19.930291Z","iopub.execute_input":"2021-12-12T16:21:19.930689Z","iopub.status.idle":"2021-12-12T16:21:19.949465Z","shell.execute_reply.started":"2021-12-12T16:21:19.930598Z","shell.execute_reply":"2021-12-12T16:21:19.948362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(df, batch_size =  16, num_workers = 2,in_vars = new_vars, target= 'ARRIVAL_DELAY'):\n    #create pytorch dataloader\n    train_subset, val_subset = train_test_split(df, test_size = 0.2)\n    train_subset = train_subset.reset_index(drop=True)\n    val_subset = val_subset.reset_index(drop=True)\n    \n    train_input = torch.tensor(train_subset[in_vars].values.astype(np.float32))\n    train_target = torch.tensor(train_subset[target].values.astype(np.float32))\n    \n    val_input = torch.tensor(val_subset[in_vars].values.astype(np.float32))\n    val_target = torch.tensor(val_subset[target].values.astype(np.float32))\n    \n    train_tensor = data_utils.TensorDataset(train_input, train_target) \n    val_tensor = data_utils.TensorDataset(val_input, val_target) \n    \n    train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n    val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size, shuffle = True)\n    \n    return train_loader, val_loader","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:21:20.294342Z","iopub.execute_input":"2021-12-12T16:21:20.294626Z","iopub.status.idle":"2021-12-12T16:21:20.306365Z","shell.execute_reply.started":"2021-12-12T16:21:20.29458Z","shell.execute_reply":"2021-12-12T16:21:20.305637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_cifar(config, inshape = len(new_vars),train_data = None, checkpoint_dir=None):\n    # train pytorch neural network\n    net = Net(inshape,l1 =config[\"l1\"], l2 =config[\"l2\"],\n             l3 =config[\"l3\"], l4 =config[\"l4\"],\n             l5 =config[\"l5\"], l6 =config[\"l6\"])\n\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if torch.cuda.device_count() > 1:\n            net = nn.DataParallel(net)\n    net.to(device)\n\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adadelta(net.parameters(), lr = config['lr'])\n\n    if checkpoint_dir:\n        model_state, optimizer_state = torch.load(\n            os.path.join(checkpoint_dir, \"checkpoint\"))\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    trainloader, valloader = load_data(train_data, batch_size = int(config[\"batch_size\"]))\n\n    for epoch in range(10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for data in trainloader:\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels[:,None])\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            #if i % 2000 == 1999:  # print every 2000 mini-batches\n            #    print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n            #                                    running_loss / epoch_steps))\n            #    running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        for data in valloader:\n            with torch.no_grad():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                \n                loss = mean_squared_error(outputs.cpu().numpy(), labels.cpu().numpy())\n                val_loss += loss\n                val_steps += 1\n\n        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, \"checkpoint\")\n            torch.save((net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=(val_loss / val_steps))\n    print(\"Finished Training\")","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:22:03.793393Z","iopub.execute_input":"2021-12-12T16:22:03.796386Z","iopub.status.idle":"2021-12-12T16:22:03.82584Z","shell.execute_reply.started":"2021-12-12T16:22:03.79634Z","shell.execute_reply":"2021-12-12T16:22:03.824835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n    config = {\n        \"l1\": tune.grid_search([1600,1400]),\n        \"l2\": tune.grid_search([1200,1000]),\n        \"l3\": tune.grid_search([800,600]),\n        \"l4\": tune.grid_search([400,300]),\n        \"l5\": tune.grid_search([200,100]),\n        \"l6\": tune.grid_search([50,25]),\n        \"lr\": tune.grid_search([1e-3,1e-4]),\n        \"batch_size\": tune.grid_search([16,32,64])\n    }\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=max_num_epochs,\n        grace_period=1,\n        reduction_factor=2)\n    reporter = CLIReporter(\n        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n    result = tune.run(\n        tune.with_parameters(train_cifar, train_data = ftrain.sample(2000)),\n        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n        progress_reporter=reporter,\n        verbose = 1)\n\n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    print(\"Best trial config: {}\".format(best_trial.config))\n    print(\"Best trial final validation loss: {}\".format(\n        best_trial.last_result[\"loss\"]))\n    print(\"Best trial final validation accuracy: {}\".format(\n        best_trial.last_result[\"accuracy\"]))\n\n    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n        if gpus_per_trial > 1:\n            best_trained_model = nn.DataParallel(best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint_dir = best_trial.checkpoint.value\n    model_state, optimizer_state = torch.load(os.path.join(\n        best_checkpoint_dir, \"checkpoint\"))\n    best_trained_model.load_state_dict(model_state)\n\n    test_acc = test_accuracy(best_trained_model, device)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n\n\n#if __name__ == \"__main__\":\n#    # You can change the number of GPUs per trial here:\n#    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:35:06.279469Z","iopub.execute_input":"2021-12-12T16:35:06.279911Z","iopub.status.idle":"2021-12-12T16:35:06.293938Z","shell.execute_reply.started":"2021-12-12T16:35:06.27985Z","shell.execute_reply":"2021-12-12T16:35:06.292291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(num_samples=1, max_num_epochs=10, gpus_per_trial=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T16:35:06.450779Z","iopub.execute_input":"2021-12-12T16:35:06.451544Z","iopub.status.idle":"2021-12-12T16:35:15.056869Z","shell.execute_reply.started":"2021-12-12T16:35:06.451501Z","shell.execute_reply":"2021-12-12T16:35:15.055399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUBMISSION","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n      def __init__(self, inshape, l1,l2,l3,l4,l5,l6):\n        super().__init__()\n        self.layers = nn.Sequential(\n          nn.Flatten(),\n          nn.Linear(inshape, l1),\n          nn.ReLU(),\n          nn.Linear(l1, l2),\n          nn.ReLU(),\n          nn.Linear(l2, l3),\n          nn.ReLU(),\n          nn.Linear(l3, l4),\n          nn.ReLU(),\n          nn.Linear(l4, l5),\n          nn.ReLU(),\n          nn.Linear(l5, l6),\n          nn.ReLU(),\n          nn.Linear(l6, 1)\n        )\n\n      def forward(self, x):\n        #x = self.flatten(x)\n        '''\n          Forward pass\n        '''\n        return self.layers(x)\n    \n    \nanet = Net(11352, 1600, 1200, 800, 300,200,50)\nanet = nn.DataParallel(anet)\nanet.load_state_dict(torch.load('../input/aml-model/best_model_dict.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(infile)\ntrain_oh = pd.get_dummies(train_data, columns = cat_cols)\nnew_vars = list(train_oh.columns[8:15]) + list(train_oh.columns[16:])\ninput_length = len(new_vars)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\nsubmit_sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T14:44:16.865396Z","iopub.execute_input":"2021-12-10T14:44:16.865688Z","iopub.status.idle":"2021-12-10T14:44:17.997789Z","shell.execute_reply.started":"2021-12-10T14:44:16.865658Z","shell.execute_reply":"2021-12-10T14:44:17.996956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\nprint(len(submit_sample))\nairports_origin = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'ORIGIN_AIRPORT'})\nairports_arrive = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'DESTINATION_AIRPORT'})\nsubmit_sample1 = submit_sample.merge(airports_origin, on = 'ORIGIN_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_origin', 'LONGITUDE' : 'LONGITUDE_origin'})\nsubmit_sample2 = submit_sample1.merge(airports_arrive, on = 'DESTINATION_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_arrival', 'LONGITUDE' : 'LONGITUDE_arrival'})\nprint(len(submit_sample2.dropna()))\nsubmit_sample2[cont_cols] = s_scaler.transform(submit_sample2[cont_cols])\nsubmit_sample2 = pd.get_dummies(submit_sample2, columns = cat_cols)\nsubmit_sample2 = submit_sample2.sort_values(by=['id'])\nsubmit_sample2.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T16:35:53.509448Z","iopub.execute_input":"2021-12-10T16:35:53.51018Z","iopub.status.idle":"2021-12-10T16:35:55.619966Z","shell.execute_reply.started":"2021-12-10T16:35:53.510133Z","shell.execute_reply":"2021-12-10T16:35:55.619056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_sample = pd.read_csv('../input/flight-delays-prediction-challeng2021/flights_test.csv')\nprint(len(submit_sample))\nairports_origin = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'ORIGIN_AIRPORT'})\nairports_arrive = airp[['IATA_CODE','LATITUDE','LONGITUDE']].rename(columns = {'IATA_CODE' : 'DESTINATION_AIRPORT'})\nsubmit_sample1 = submit_sample.merge(airports_origin, on = 'ORIGIN_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_origin', 'LONGITUDE' : 'LONGITUDE_origin'})\nsubmit_sample2 = submit_sample1.merge(airports_arrive, on = 'DESTINATION_AIRPORT').rename(columns = {'LATITUDE' : 'LATITUDE_arrival', 'LONGITUDE' : 'LONGITUDE_arrival'})\nprint(len(submit_sample2.dropna()))\nprint(submit_sample2.columns)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:04:36.023035Z","iopub.execute_input":"2021-12-16T20:04:36.023546Z","iopub.status.idle":"2021-12-16T20:04:37.752466Z","shell.execute_reply.started":"2021-12-16T20:04:36.023508Z","shell.execute_reply":"2021-12-16T20:04:37.751708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_cols = submit_sample2.columns[10:]\nprint(cont_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:08:18.79449Z","iopub.execute_input":"2021-12-16T20:08:18.795067Z","iopub.status.idle":"2021-12-16T20:08:18.79964Z","shell.execute_reply.started":"2021-12-16T20:08:18.795028Z","shell.execute_reply":"2021-12-16T20:08:18.798686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_cols = submit_sample2.columns[10:]\nsubmit_sample2[cont_cols] = s_scaler.transform(submit_sample2[cont_cols])\nprint(cont_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:12:08.152361Z","iopub.execute_input":"2021-12-16T20:12:08.153102Z","iopub.status.idle":"2021-12-16T20:12:08.244501Z","shell.execute_reply.started":"2021-12-16T20:12:08.153043Z","shell.execute_reply":"2021-12-16T20:12:08.243569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh_dict['AIRLINE']","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:14:19.477749Z","iopub.execute_input":"2021-12-16T20:14:19.478313Z","iopub.status.idle":"2021-12-16T20:14:19.503357Z","shell.execute_reply.started":"2021-12-16T20:14:19.478274Z","shell.execute_reply":"2021-12-16T20:14:19.502562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_samp = submit_sample2.sample(50)\nin_list = []\nfor i, row in sub_samp.iterrows():\n    cont_vals = list(row[cont_cols])\n    for cat in oh_dict:\n        cat_val = row[cat]\n        cat_un = list(oh_dict[cat])\n        cat_zeros = np.zeros(len(cat_un))\n        cat_ind = cat_un.index(cat_val)\n        cat_zeros[cat_ind] = 1\n        cont_vals += list(cat_zeros)\n    in_list.append(cont_vals)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:16:48.706231Z","iopub.execute_input":"2021-12-16T20:16:48.706788Z","iopub.status.idle":"2021-12-16T20:16:48.851718Z","shell.execute_reply.started":"2021-12-16T20:16:48.706747Z","shell.execute_reply":"2021-12-16T20:16:48.850973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(in_list[0])\nprint(in_list[0][:15])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:18:10.658123Z","iopub.execute_input":"2021-12-16T20:18:10.658378Z","iopub.status.idle":"2021-12-16T20:18:10.663002Z","shell.execute_reply.started":"2021-12-16T20:18:10.658349Z","shell.execute_reply":"2021-12-16T20:18:10.662259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tensorflow","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn import preprocessing\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom tensorflow.keras.layers import Dense, Conv2D , SeparableConv2D, MaxPool2D, Flatten , Dropout , BatchNormalization\nfrom sklearn.metrics import r2_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom scipy.stats import zscore\nfrom bayes_opt import BayesianOptimization\nimport keras_tuner as kt\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(600, input_shape=input_shape, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(450, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(300, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(200, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(100, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n# Compile model\nmodel.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())","metadata":{"execution":{"iopub.status.busy":"2021-12-10T15:11:21.069861Z","iopub.execute_input":"2021-12-10T15:11:21.070133Z","iopub.status.idle":"2021-12-10T15:11:23.580061Z","shell.execute_reply.started":"2021-12-10T15:11:21.070103Z","shell.execute_reply":"2021-12-10T15:11:23.579364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(ftrain[new_vars], ftrain['ARRIVAL_DELAY'], epochs = 10, batch_size = 32, validation_split = 0.15)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T16:00:29.528135Z","iopub.execute_input":"2021-12-10T16:00:29.528774Z","iopub.status.idle":"2021-12-10T16:33:54.933703Z","shell.execute_reply.started":"2021-12-10T16:00:29.528728Z","shell.execute_reply":"2021-12-10T16:33:54.932994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_cat_cols = ['DAY','DAY_OF_WEEK','AIRLINE', 'FLIGHT_NUMBER','TAIL_NUMBER']\nfor j in range(3):\n    cat_cols = all_cat_cols[:3+j]\n    print(cat_cols)\n    dir_name = 'cat_col' + str(j)\n\n    train_oh = pd.get_dummies(small_train, columns = cat_cols)\n\n    new_vars = list(train_oh.columns[7:14]) + list(train_oh.columns[16:])\n    input_length = len(new_vars)\n    input_shape = (input_length,)\n    \n    def model_builder(hp, j= j,input_shape = input_shape):\n        model = keras.Sequential()\n        model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n\n        for i in range(6):\n            model.add(tf.keras.layers.Dense(units=hp.Choice('units_' + str(i), \n                                                            values=[1200,1000,800,700,\n                                                                    600,500,400,300,\n                                                                    200,100,50,25][i*2:i*2+2]),\n                                            activation='relu'))\n\n        model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n\n        hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n\n        model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=hp_learning_rate),\n                loss='mean_squared_error',\n                metrics=[\n                        'MeanSquaredError'\n                        ]\n                     )\n\n        return model\n    \n    tuner = kt.Hyperband(model_builder,\n                     objective='val_mean_squared_error',\n                     max_epochs=60,\n                     factor=3,\n                     directory=dir_name ,\n                     project_name='intro_to_kt')\n\n    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n    \n    tuner.search(train_oh[new_vars], \n             train_oh['ARRIVAL_DELAY'], \n             epochs=60, \n             validation_split=0.15,\n             callbacks = [stop_early])\n\n    # Get the optimal hyperparameters\n    best_hp=tuner.get_best_hyperparameters(num_trials=1)[0]\n    model = tuner.hypermodel.build(best_hp)\n    model_name = 'best_model_' + str(j)\n    model.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:04:34.401446Z","iopub.execute_input":"2021-12-11T18:04:34.401785Z","iopub.status.idle":"2021-12-11T18:54:52.620268Z","shell.execute_reply.started":"2021-12-11T18:04:34.40175Z","shell.execute_reply":"2021-12-11T18:54:52.618685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_builder(hp, input_shape = input_shape):\n    model = keras.Sequential()\n    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n    \n    for i in range(6):\n        model.add(tf.keras.layers.Dense(units=hp.Choice('units_' + str(i), \n                                                        values=[600,400,300,200,100,50,25][i:i+2]),\n                                        activation='relu'))\n\n    model.add(Dense(1, kernel_initializer='normal', activation = 'linear'))\n\n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n\n    model.compile(optimizer=keras.optimizers.Adadelta(learning_rate=hp_learning_rate),\n            loss='mean_squared_error',\n            metrics=[\n                    'MeanSquaredError'\n                    ],\n            steps_per_execution=8\n                 )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-08T15:45:18.173513Z","iopub.execute_input":"2021-12-08T15:45:18.173982Z","iopub.status.idle":"2021-12-08T15:45:18.182885Z","shell.execute_reply.started":"2021-12-08T15:45:18.173943Z","shell.execute_reply":"2021-12-08T15:45:18.182018Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kt.Hyperband(model_builder,\n                     objective='val_mean_squared_error',\n                     max_epochs=50,\n                     factor=3,\n                     directory=dir_name ,\n                     project_name='intro_to_kt')\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T15:45:18.184871Z","iopub.execute_input":"2021-12-08T15:45:18.185138Z","iopub.status.idle":"2021-12-08T15:45:20.763903Z","shell.execute_reply.started":"2021-12-08T15:45:18.185103Z","shell.execute_reply":"2021-12-08T15:45:20.763082Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"tuner.search(train[new_vars], \n             train['ARRIVAL_DELAY'], \n             epochs=50, \n             validation_split=0.15,\n             callbacks = [stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-08T15:45:29.252266Z","iopub.execute_input":"2021-12-08T15:45:29.252533Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_model(train_data, val_data, input_size = input_shape, input_columns = new_vars):\n    \n    def dense_training(train_df, val_df, list_of_params, input_size, input_columns, \n                       epochs, remove_outliers, batch_size, target_column = 'ARRIVAL_DELAY'):\n        \n        if remove_outliers:\n            z_scores = zscore(train_df[target_column])\n            abs_z_scores = np.abs(z_scores)\n            filtered_entries = (abs_z_scores < 3)\n            train_df = train_df[filtered_entries]\n        \n        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n        \n        model = build_model(list_of_params, input_size)\n        result = model.fit(train_df[input_columns], train_df[target_column], \n                        epochs=epochs, batch_size=batch_size, \n                        callbacks=[stop_early],\n                        validation_split=(val_df[input_columns],val_df[target_column]))\n        return result.history['val_loss']\n    \n                           \n    optimizer = BayesianOptimization(\n    f=dense_training,\n    pbounds={\n        'train_df': train_data,\n        'val_df' : val_data,\n        'input_size' : input_size,\n        'input_columns' : input_columns,\n        'remove outliers' : [True, False],\n        'list_of_params': [[200,100,50,25],[400,200,100,50,25],[800,400,200,100,50,25],[600,450,300,150,75,25]], \n        'epochs': (10,20,50,80,100),\n        'batch_size': (16,32,64)\n            },\n    random_state = 12,\n    verbose=2\n    )\n                           \n    optimizer.maximize(n_iter=10)\n\n    print(\"Final result:\", optimizer.max)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T11:35:22.76004Z","iopub.execute_input":"2021-12-08T11:35:22.760684Z","iopub.status.idle":"2021-12-08T11:35:22.773542Z","shell.execute_reply.started":"2021-12-08T11:35:22.760638Z","shell.execute_reply":"2021-12-08T11:35:22.772642Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tuner = kt.Hyperband(build_model,\n                     objective=\"val_accuracy\",\n                     max_epochs=100,\n                     factor=3,\n                     hyperband_iterations=10,\n                     directory=\"kt_dir\",\n                     project_name=\"kt_hyperband\",)","metadata":{}}]}